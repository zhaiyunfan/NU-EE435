{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOJS4unEuwiJ"
      },
      "source": [
        "# Coding CNNs from Scratch with Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAglL7KZu3ge"
      },
      "source": [
        "In this assignment you will code a famous CNN architecture AlexNet (https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html) to classify images from the CIFAR10 dataset (https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 10 classes of natural images such as vehicles or animals. AlexNet is a landmark architecture because it was one of the first extremely deep CNNs trained on GPUs, and achieved state-of-the-art performance in the ImageNet challenge in 2012.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv77OEtlxuP8"
      },
      "source": [
        "A lot of code will already be written to familiarize yourself with PyTorch, but you will have to fill in parts that will apply your knowledge of CNNs. Additionally, there are some numbered questions that you must answer either in a separate document, or in this notebook. Some questions may require you to do a little research. To type in the notebook, you can insert a text cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr5aNOagwwm5"
      },
      "source": [
        "Let's start by installing PyTorch and the torchvision package below. Due to the size of the network, you will have to run on a GPU. So, click on the Runtime dropdown, then Change Runtime Type, then GPU for the hardware accelerator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HXnfRg4IulGd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: torchvision in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (0.18.0)\n",
            "Requirement already satisfied: torchaudio in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (2.3.0+cu118)\n",
            "Requirement already satisfied: filelock in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (2024.5.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torch) (2021.4.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from torchvision) (10.3.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\zhaiy\\iclouddrive\\northwestern\\lecture\\ee 435\\homework\\nu-ee435\\.venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VtC0KJcdufBE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# device = torch.device('cuda')\n",
        "\n",
        "# print(device)\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu May 16 17:50:55 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 546.65                 Driver Version: 546.65       CUDA Version: 12.3     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3060 Ti   WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| 30%   37C    P5              28W / 200W |    971MiB /  8192MiB |      6%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|    0   N/A  N/A      1496    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A      2928    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A      3084    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7904    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
            "|    0   N/A  N/A      9560    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
            "|    0   N/A  N/A      9592    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     13260    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
            "|    0   N/A  N/A     13704    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     14152    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A     18628    C+G   C:\\Program Files\\Tencent\\QQNT\\QQ.exe      N/A      |\n",
            "|    0   N/A  N/A     18892    C+G   ...9\\extracted\\runtime\\WeChatAppEx.exe    N/A      |\n",
            "|    0   N/A  N/A     20544    C+G   ...al\\Discord\\app-1.0.9147\\Discord.exe    N/A      |\n",
            "|    0   N/A  N/A     21712    C+G   ...\\Local\\slack\\app-4.38.125\\slack.exe    N/A      |\n",
            "|    0   N/A  N/A     21720    C+G   ...nzyj5cx40ttqa\\iCloud\\iCloudHome.exe    N/A      |\n",
            "|    0   N/A  N/A     22440    C+G   ...meCenter\\dlls\\wgc_renderer_host.exe    N/A      |\n",
            "|    0   N/A  N/A     23364    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A     23560    C+G   C:\\Windows\\System32\\mmgaserver.exe        N/A      |\n",
            "|    0   N/A  N/A     24252    C+G   ...yj5cx40ttqa\\iCloud\\iCloudPhotos.exe    N/A      |\n",
            "|    0   N/A  N/A     25296    C+G   ...nipaste-2.8.5-Beta-x64\\Snipaste.exe    N/A      |\n",
            "|    0   N/A  N/A     27160    C+G   ...esktop\\app-3.3.14\\GitHubDesktop.exe    N/A      |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DML-S0AX-_o"
      },
      "source": [
        "### 1. In the following cell, we are employing something called \"data augmentation\" with random horizontal and vertical flips. So when training data is fed into the network, it is ranadomly transformed. What are advantages of this?\n",
        "\n",
        "### 2. We normalize with the line transforms.Normalize((0.5,), (0.5,)). What are the benefits of normalizing data?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Data augmentation, such as random horizontal and vertical flips, increases the diversity of the training dataset without collecting more data. This technique helps the model generalize better to new, unseen data by preventing overfitting. By presenting various transformations of the images, the model learns to recognize objects in different orientations, which makes it more robust. Moreover, this approach helps the model become invariant to these transformations, enhancing its ability to perform well in real-world scenarios where objects may appear in different orientations.\n",
        "\n",
        "2. Normalizing data with transforms.Normalize((0.5,), (0.5,)) scales the input data to have a mean of 0 and a standard deviation of 1. This standardization is crucial for several reasons. First, it speeds up the convergence of gradient descent during training by ensuring that all input features are on a similar scale. Second, it helps avoid issues related to numerical instability and exploding/vanishing gradients, making the training process more stable. Finally, normalization ensures that the network learns weights more effectively, leading to better performance and faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eruiC4sAufBL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "40000 10000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "400"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split\n",
        "from math import ceil\n",
        "\n",
        "BATCH_SIZE = 100\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.RandomHorizontalFlip(p=0.5),\n",
        "     transforms.RandomVerticalFlip(p=0.5),\n",
        "     transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "\n",
        "torch.manual_seed(43)\n",
        "val_size = 10000\n",
        "train_size = len(trainset) - val_size\n",
        "\n",
        "\n",
        "train_ds, val_ds = random_split(trainset, [train_size, val_size])\n",
        "print(len(train_ds), len(val_ds))\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "num_steps =  ceil(len(train_ds) / BATCH_SIZE)\n",
        "num_steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "NLzuKuJxufBM"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_ds, BATCH_SIZE, shuffle=True, drop_last = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_ds, BATCH_SIZE)\n",
        "test_loader = torch.utils.data.DataLoader(testset, BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rT3aDd7aVLm"
      },
      "source": [
        "You can insert an integer  into the code trainset[#insert integer] to visualize images from the training set. Some of the images might look weird because they have been randomly flipped according to our data augmentation scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "wV-W2b6eZaoG",
        "outputId": "59486fe1-2667-4bf1-b7af-84a647899b83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label (numeric): 6\n",
            "Label (textual): frog\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "img, label = trainset[0]\n",
        "plt.imshow((img.permute((1, 2, 0))+1)/2)\n",
        "print('Label (numeric):', label)\n",
        "print('Label (textual):', classes[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLQM7ODamEm"
      },
      "source": [
        "Now comes the fun part. You will have to put in the correct parameters into different torch.nn functions in order to convolve and downsample the image into the correct dimensionality for classification. Think of it as a puzzle. You will insert the parameters where there is a comment #TODO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ohY_5zoBufBN"
      },
      "outputs": [],
      "source": [
        "class Discriminator(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = 3,\n",
        "                      out_channels = 64,\n",
        "                      kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(64, 192, kernel_size=3,\n",
        "                      padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(192,\n",
        "                      384,\n",
        "                      kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "        )\n",
        "\n",
        "        # Calculate the number of features after the convolutional layers\n",
        "        self._initialize_linear_input_features()\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(self.num_features, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(4096, 10),\n",
        "        )\n",
        "\n",
        "    def _initialize_linear_input_features(self):\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(\n",
        "                1, 3, 32, 32\n",
        "            ) # specify the input size of our network\n",
        "            features = self.features(dummy_input)\n",
        "            self.num_features = features.view(1, -1).size(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        # we must flatten our feature maps before feeding into fully connected layers\n",
        "        x = x.contiguous().view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw2ZfQfAeZum"
      },
      "source": [
        "Below we are initializing our model with a weight scheme."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AlUFRnhZufBN"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Discriminator()\n",
        "\n",
        "def weights_init(m):\n",
        "\n",
        "    classname = m.__class__.__name__\n",
        "\n",
        "    if classname.find('Conv') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# Initialize Models\n",
        "net = net.to(device)\n",
        "\n",
        "net.apply(weights_init)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFBRnApFfZUr"
      },
      "source": [
        "# 3. Notice above in our network architecture, we have what are called \"Dropout\" layers. What is the point of these?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Dropout layers are a regularization technique used in neural networks to prevent overfitting. During training, dropout randomly \"drops\" a fraction of neurons, setting their output to zero. This forces the network to learn more robust features, as it can't rely on any single neuron. By doing this, the network generalizes better to unseen data, improving its performance on the test set. In essence, dropout helps the network avoid becoming too tailored to the training data and instead learn patterns that are more widely applicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqzRwTN-febi"
      },
      "source": [
        "Defining our cost/loss function, which is cross-entropy loss. We also define our optimizer with hyperparameters: learning rate and betas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sNvPJc_PufBN"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(\n",
        "    net.parameters(),\n",
        "    lr=0.0002,\n",
        "    betas = (0.5, 0.999)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JR6xLm7KiR3p"
      },
      "source": [
        "Below we actually train our network. Run for just 10 epochs. It takes some time. Wherever there is the comment #TODO, you must insert code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2VwEjNs3ufBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "E:1, Train Loss:1.9267987021803856\n",
            "Accuracy of 10000 val images: 0.3369\n",
            "Val Loss: 0.43540037512779234\n",
            "E:2, Train Loss:1.577186998128891\n",
            "Accuracy of 10000 val images: 0.4601\n",
            "Val Loss: 0.3608863279223442\n",
            "E:3, Train Loss:1.402827124297619\n",
            "Accuracy of 10000 val images: 0.4822\n",
            "Val Loss: 0.3492458724975586\n",
            "E:4, Train Loss:1.2843626663088799\n",
            "Accuracy of 10000 val images: 0.5251\n",
            "Val Loss: 0.3230030071735382\n",
            "E:5, Train Loss:1.184310462921858\n",
            "Accuracy of 10000 val images: 0.5722\n",
            "Val Loss: 0.2963975059986115\n",
            "E:6, Train Loss:1.1096938508749008\n",
            "Accuracy of 10000 val images: 0.591\n",
            "Val Loss: 0.28300827845931054\n",
            "E:7, Train Loss:1.0468915204703808\n",
            "Accuracy of 10000 val images: 0.597\n",
            "Val Loss: 0.28580799102783205\n",
            "E:8, Train Loss:0.9906741769611835\n",
            "Accuracy of 10000 val images: 0.5958\n",
            "Val Loss: 0.2820750530064106\n",
            "E:9, Train Loss:0.93946278616786\n",
            "Accuracy of 10000 val images: 0.6284\n",
            "Val Loss: 0.2618029835820198\n",
            "E:10, Train Loss:0.8902475713193416\n",
            "Accuracy of 10000 val images: 0.6444\n",
            "Val Loss: 0.2531955054402351\n",
            "E:11, Train Loss:0.8488996098935604\n",
            "Accuracy of 10000 val images: 0.6499\n",
            "Val Loss: 0.2508987531065941\n",
            "E:12, Train Loss:0.805747145563364\n",
            "Accuracy of 10000 val images: 0.6618\n",
            "Val Loss: 0.24271347984671593\n",
            "E:13, Train Loss:0.7709059642255306\n",
            "Accuracy of 10000 val images: 0.644\n",
            "Val Loss: 0.2526468175649643\n",
            "E:14, Train Loss:0.7262264236807823\n",
            "Accuracy of 10000 val images: 0.6678\n",
            "Val Loss: 0.2411696757376194\n",
            "E:15, Train Loss:0.695579390078783\n",
            "Accuracy of 10000 val images: 0.6826\n",
            "Val Loss: 0.2359524643421173\n",
            "E:16, Train Loss:0.6570892688632012\n",
            "Accuracy of 10000 val images: 0.6837\n",
            "Val Loss: 0.22907041296362876\n",
            "E:17, Train Loss:0.6298246267437935\n",
            "Accuracy of 10000 val images: 0.6801\n",
            "Val Loss: 0.23828564956784248\n",
            "E:18, Train Loss:0.5942474261671304\n",
            "Accuracy of 10000 val images: 0.6876\n",
            "Val Loss: 0.23742693290114403\n",
            "E:19, Train Loss:0.5644176856428385\n",
            "Accuracy of 10000 val images: 0.689\n",
            "Val Loss: 0.22997823387384414\n",
            "E:20, Train Loss:0.5388002382218837\n",
            "Accuracy of 10000 val images: 0.6998\n",
            "Val Loss: 0.23052080050110818\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(20):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)     #pass input data into network to get outputs\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()  #calculate gradients\n",
        "        optimizer.step() #take gradient descent step\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(\"E:{}, Train Loss:{}\".format(\n",
        "                epoch+1,\n",
        "                running_loss / num_steps\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # validation\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in val_loader:\n",
        "            # TODO: load images and labels from validation loader\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(images)  # run forward pass\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            loss = criterion(outputs, labels)       #calculate validation loss\n",
        "            val_loss += loss.item()\n",
        "    val_loss /=num_steps\n",
        "    print('Accuracy of 10000 val images: {}'.format( correct / total))\n",
        "    print('Val Loss: {}'.format( val_loss))\n",
        "\n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sKf8Lu-mGYd"
      },
      "source": [
        "## 4. If we train for more epochs, our accuracy/performance will increase. What happens if we train for too long though? What method can be employed to mitigate this?\n",
        "\n",
        "## 5. Try increasing learning rate and look at the metrics for training and validation data? What do you notice? Why do think this is happening?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Training for too long can lead to overfitting, where the model learns the noise and details in the training data to the extent that it performs poorly on new, unseen data. To mitigate this, we can use techniques like early stopping, which monitors the validation performance and stops training when performance no longer improves. Regularization methods like dropout and weight decay can also help prevent overfitting by adding constraints that keep the model from becoming too complex.\n",
        "\n",
        "5. Increasing the learning rate often leads to faster convergence initially, but it can also cause the training process to become unstable. You might notice that the training loss fluctuates or increases, and the validation performance might degrade. This happens because a high learning rate can cause the model to overshoot the optimal weights, making it difficult for the gradient descent algorithm to settle into a minimum. Reducing the learning rate can help achieve more stable and reliable convergence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz31Vm2XmM7p"
      },
      "source": [
        "We can see the performance on the testing set now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SNLMA4oIufBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of 10000 test images: 0.6971\n"
          ]
        }
      ],
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of 10000 test images: {}'.format( correct / total))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
